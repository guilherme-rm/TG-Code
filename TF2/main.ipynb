{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy import sparse as sp\n",
    "from utils import *\n",
    "from path_model import LRGCN\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix,roc_auc_score\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "import tf_slim as slim\n",
    "from scipy import sparse\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "from load_data import load_data\n",
    "import sys\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.flags.DEFINE_integer('num_epochs', 25, 'number of epochs to train')\n",
    "tf.compat.v1.flags.DEFINE_integer('batch_size', 1, 'batch size to train in one step')\n",
    "tf.compat.v1.flags.DEFINE_integer('labels', 1, 'number of label classes')\n",
    "tf.compat.v1.flags.DEFINE_integer('word_pad_length', 4438, 'word pad length for training')\n",
    "tf.compat.v1.flags.DEFINE_integer('decay_step', 500, 'decay steps')\n",
    "tf.compat.v1.flags.DEFINE_float('learn_rate', 1e-2, 'learn rate for training optimization')\n",
    "tf.compat.v1.flags.DEFINE_boolean('train', False, 'train mode FLAG')\n",
    "tf.compat.v1.flags.DEFINE_string('f','','')\n",
    "\n",
    "FLAGS = tf.compat.v1.flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = FLAGS.num_epochs\n",
    "batch_size = FLAGS.batch_size\n",
    "tag_size = FLAGS.labels\n",
    "word_pad_length = FLAGS.word_pad_length\n",
    "feature_dimension = 2\n",
    "lr = FLAGS.learn_rate\n",
    "window_size = 24\n",
    "num_path = 200\n",
    "max_path_len = 49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "placeholders = {\n",
    "    'support': [tf.compat.v1.sparse_placeholder(tf.float32) for _ in range(window_size)],\n",
    "    'features':tf.compat.v1.placeholder(tf.float32, shape=(window_size,word_pad_length,feature_dimension)),\n",
    "    'labels': tf.compat.v1.placeholder(tf.float32, shape=(num_path,)),\n",
    "    'labels_mask': tf.compat.v1.placeholder(tf.int32,shape=(num_path,word_pad_length)),\n",
    "    'dropout': tf.compat.v1.placeholder_with_default(0., shape=()),\n",
    "    'path_node_index_array': tf.compat.v1.placeholder(tf.int32,shape=(num_path, max_path_len)),\n",
    "    'num_features_nonzero': tf.compat.v1.placeholder(tf.int32)  # helper variable for sparse dropout\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LRGCN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.compat.v1.Session() as sess:\n",
    "  # build graph\n",
    "  model.build_graph(n=word_pad_length,placeholders = placeholders,d =feature_dimension)\n",
    "  # Downstream Application\n",
    "  with tf.compat.v1.variable_scope('DownstreamApplication'):\n",
    "    global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "    learn_rate = tf.compat.v1.train.exponential_decay(lr, global_step, FLAGS.decay_step, 0.98, staircase=True)\n",
    "    labels = placeholders['labels']\n",
    "    Mask = placeholders['labels_mask']\n",
    "    initializer = tf.keras.initializers.he_normal()\n",
    "    print(\"---\\n model.H2 shape: {}\\n----\".format(model.H2.shape))\n",
    "    zero_padding = tf.constant(0.0, shape=[1, 8])\n",
    "    rnn_input = tf.nn.embedding_lookup(tf.concat([model.H2, zero_padding], 0), placeholders['path_node_index_array'])\n",
    "    sub_w11 = tf.compat.v1.get_variable(name='sub_w11',shape=(32,8),initializer=initializer)\n",
    "    sub_w22 = tf.compat.v1.get_variable(name='sub_w22',shape=(8,32),initializer=initializer)\n",
    "    lstm_cell = tf.compat.v1.nn.rnn_cell.LSTMCell(8)\n",
    "    outputs,last_states=   tf.compat.v1.nn.dynamic_rnn(cell = lstm_cell,inputs = rnn_input, dtype = tf.float32)\n",
    "    print(\"---\\n outputs shape: {}\\n----\".format(outputs.shape))\n",
    "    outputs_trans = tf.transpose(outputs, perm=[0, 2, 1])\n",
    "    print(\"---\\n outputs_trans shape: {}\\n----\".format(outputs_trans.shape))    \n",
    "    sub_w11_stack = tf.tile(tf.expand_dims(sub_w11, 0), [num_path, 1, 1])\n",
    "    sub_w22_stack = tf.tile(tf.expand_dims(sub_w22, 0), [num_path, 1, 1])\n",
    "    attention_path = tf.nn.softmax(tf.matmul(sub_w22_stack, tf.tanh(tf.matmul(sub_w11_stack, outputs_trans))))\n",
    "    print(\"---\\n attention_path shape: {}\\n----\".format(attention_path.shape))\n",
    "\n",
    "    path_output = tf.matmul(attention_path, outputs)\n",
    "    print(\"---\\n path_output shape: {}\\n----\".format(path_output.shape))\n",
    "    path_output = tf.reshape(path_output, [num_path, 1, 8*8])\n",
    "    print(\"---\\n path_output shape: {}\\n----\".format(path_output.shape))\n",
    "    initializer = tf.keras.initializers.he_normal()\n",
    "    fc_weights = tf.compat.v1.get_variable(name='fc_weights',shape=(64,1),initializer=initializer)\n",
    "    fc_weights_stack = tf.tile(tf.expand_dims(fc_weights, 0), [num_path, 1, 1])\n",
    "    logits = tf.reshape(tf.matmul(path_output,fc_weights_stack),[-1])\n",
    "    print(\"---\\n logits shape: {}\\n----\".format(logits.shape))\n",
    "    loss = tf.nn.weighted_cross_entropy_with_logits(targets=labels, logits=logits,pos_weight = 3.) \n",
    "    loss = tf.reduce_mean(loss)\n",
    "    params = tf.compat.v1.trainable_variables()\n",
    "    optimizer = tf.compat.v1.train.AdamOptimizer(learn_rate)\n",
    "    grad_and_vars = tf.gradients(loss, params)\n",
    "    clipped_gradients, _ = tf.clip_by_global_norm(grad_and_vars, 1)\n",
    "    opt = optimizer.apply_gradients(zip(clipped_gradients, params), global_step=global_step)\n",
    "    print(\"HERE3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_summary():\n",
    "    model_vars = tf.trainable_variables()\n",
    "    slim.model_analyzer.analyze_vars(model_vars, print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
